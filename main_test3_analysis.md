# main_test3.py 分析レポート

## 概要
`main_test3.py` は、Streamlitを使用したローカルLLMチャットアプリケーションです。Ollamaをバックエンドとして使用し、RAG (Retrieval-Augmented Generation) 機能を実装しています。ユーザーはWordファイル(.docx)をアップロードし、その内容に基づいてAIと対話することができます。

## 主な機能

1.  **チャットインターフェース**:
    *   Streamlitを使用したチャットUI。
    *   会話履歴の表示と保存。
    *   ストリーミング応答の表示。

2.  **RAG (検索拡張生成)**:
    *   ユーザーの質問に関連するドキュメントのチャンクを検索。
    *   検索結果をコンテキストとしてプロンプトに組み込み、回答を生成。

3.  **ドキュメント処理**:
    *   Wordファイル (.docx) のアップロードとテキスト抽出。
    *   テキストのチャンク分割 (サイズ: 200文字, オーバーラップ: 50文字)。
    *   ベクトルデータベース (ChromaDB) への保存。

4.  **設定 (サイドバー)**:
    *   モデル名の指定 (デフォルト: `llama3.1:8b`)。
    *   Temperature (生成の多様性) の調整。
    *   システムプロンプトのカスタマイズ。
    *   インデックス作成と会話リセット機能。

## 技術スタックと依存ライブラリ

*   **UIフレームワーク**: `streamlit`
*   **LLMクライアント**: `openai` (Ollamaのエンドポイントを使用)
*   **ベクトルデータベース**: `chromadb`
*   **ドキュメント解析**: `python-docx`
*   **HTTPリクエスト**: `requests` (埋め込み生成用)
*   **バックエンドAI**: Ollama (ローカルサーバー)

## コード構成の詳細

### 1. 初期設定とデータベース
*   **ChromaDB**: `./chroma_db` ディレクトリに永続化データを保存。
*   **コレクション**: `local_docs` という名前で作成または取得。

### 2. 主要関数
*   `ollama_embed(text)`:
    *   `http://localhost:12000/api/embeddings` にPOSTリクエストを送信。
    *   モデル `nomic-embed-text` を使用してテキストをベクトル化。
*   `load_word_document(file)`:
    *   `python-docx` を使用してWordファイルからテキストを抽出。
*   `split_text(text)`:
    *   単純な文字数ベースの分割 (Chunk Size: 200, Overlap: 50)。

### 3. メインロジック
*   **インデックス作成**:
    *   アップロードされたファイルを読み込み -> 分割 -> ベクトル化 -> ChromaDBに保存。
*   **チャット処理**:
    *   ユーザー入力 -> ベクトル化 -> ChromaDBから類似度検索 (上位2件)。
    *   検索結果がある場合、コンテキストを含めたプロンプトを作成。
    *   `OpenAI` クライアント (`base_url="http://localhost:12000/v1"`) を介してLLMにリクエスト。
    *   回答をストリーミング形式で表示。

## 改善点・注意点

1.  **ハードコードされた設定**:
    *   Ollamaのポート番号が `12000` に固定されています (標準は `11434` の場合が多い)。
    *   埋め込みモデルが `nomic-embed-text` に固定されています。

2.  **テキスト分割**:
    *   単純な文字数での分割のため、文の途中で切れる可能性があります。`LangChain` などの再帰的なテキストスプリッターの使用を検討すると精度が向上する可能性があります。

3.  **エラーハンドリング**:
    *   Ollamaサーバーが起動していない場合や、APIリクエストが失敗した場合のエラー処理が明示されていません。

4.  **プロンプト**:
    *   コンテキストを注入する際、検索結果がない場合のフォールバック処理は実装されていますが、プロンプトエンジニアリングの余地があります。
